{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nerfg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cN9ktg4uFopo",
        "colab_type": "code",
        "outputId": "b095ac2b-96ca-4f4f-b6c7-40f3bf3432a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "#nltk.download('punkt') # Download before running the code.  \n",
        "\n",
        "S = requests.Session()\n",
        "URL = \"https://nl.wikipedia.org/w/api.php\"\n",
        "\n",
        "f = open(\"firstsections.txt\", \"w\")\n",
        "g = open(\"sws.txt\", \"w\")\n",
        "\n",
        "# Gives a list of all articles inside a Wikipedia category. \n",
        "def getCategoryList(category):\n",
        "  listArticlesCategory = []\n",
        "  PARAMS = {\n",
        "      'action': \"query\",\n",
        "      'list': \"categorymembers\",\n",
        "      'cmtitle': \"Category:\" + category,\n",
        "      'cmlimit': 440, # Pakt maximaal de eerste 440 items van een categorie.\n",
        "      'cmprop':'title|sortkey',\n",
        "      #'cmstarthexsortkey': '', If you fill in the sortkey, the code will begin running at that sortkey. \n",
        "      'format': \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  for element in data['query']['categorymembers']:\n",
        "    # We do not want Wikipedia categories, only the articles inside a category.\n",
        "    if not element['title'].startswith('Categorie:'): \n",
        "      listArticlesCategory.append(element['title'])\n",
        "  \n",
        "  return listArticlesCategory\n",
        "\n",
        "# Gives a list of the parent categories from a specific Wikipedia article. \n",
        "def getParentCat(subject):\n",
        "  parentCat = []\n",
        "  PARAMS = {\n",
        "      \"action\":\"query\",\n",
        "      \"format\":\"json\", \n",
        "      \"titles\":subject,\n",
        "      \"prop\":\"categories\",\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  for value in data[\"query\"][\"pages\"].values():\n",
        "    if 'categories' in value: # Not all Wikipedia articles have a parent category.\n",
        "      for dictionary in value['categories']:\n",
        "        if not dictionary['title'].startswith('Categorie:Wikipedia:'): \n",
        "          # Wikipedia also knows hidden categories, we do not want those.\n",
        "          cat = dictionary['title']\n",
        "          parentCat.append(cat)         \n",
        "      return parentCat\n",
        "    else:\n",
        "      return subject + \" does not have any parent categories.\"\n",
        "\n",
        "# Given a certain Wikipedia article, will return a list of \n",
        "# sentences containing the title of that Wikipedia article.\n",
        "def getSentencesWithSubject(subject):\n",
        "  allSections = \"\"\n",
        "  numberSections = ['0']\n",
        "\n",
        "  PARAMS = {\n",
        "      'action': 'parse',\n",
        "      'page': subject,\n",
        "      'prop': 'sections',\n",
        "      'format': 'json'\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  # Retrieves all sections.\n",
        "  if not data['parse']['sections'] == []:\n",
        "    for sectionnumber in data['parse']['sections']:\n",
        "      if sectionnumber['index'] != '':\n",
        "        numberSections.append(sectionnumber['index'])\n",
        "        for indexnumber in numberSections:\n",
        "          PARAMS = {\n",
        "          'action': \"parse\",\n",
        "          'prop': \"wikitext\",\n",
        "          'section': indexnumber,\n",
        "          'page': subject,\n",
        "          'format': \"json\"\n",
        "          }\n",
        "\n",
        "          R = S.get(url=URL, params=PARAMS)\n",
        "          data = R.json()\n",
        "\n",
        "          nextSection = data['parse']['wikitext']['*']\n",
        "          nextSection = cleanSection(nextSection)\n",
        "          # The title of the arcticle is also a NE, so all occurences will be placed inside [[]].  \n",
        "          nextSection = re.sub(re.escape(subject), \"[[\" + subject + \"]]\", nextSection)\n",
        "          \n",
        "          allSections += nextSection + \" \"\n",
        "  else:\n",
        "    PARAMS = {\n",
        "    'action': \"parse\",\n",
        "    'prop': \"wikitext\",\n",
        "    'section': 0,\n",
        "    'page': subject,\n",
        "    'format': \"json\"\n",
        "    }\n",
        "\n",
        "    R = S.get(url=URL, params=PARAMS)\n",
        "    data = R.json()\n",
        "    allSections = data['parse']['wikitext']['*']\n",
        "    allSections = cleanSection(allSections)\n",
        "    \n",
        "    allSections = re.sub(re.escape(subject), \"[[\" + subject + \"]]\", allSections)\n",
        "      \n",
        "  # Makes a list of all sentences containing the subject,\n",
        "  # where links take the form of [[c]] or [[a|b]]. \n",
        "  # An example of [[a|b]] is [[China|Chinese]], in the text it is displayed 'Chinese',\n",
        "  # but the link will go to the Wikipedia article 'China'. \n",
        "  sentencesWithSubjectLine = []\n",
        "  for sentence in tokenize.sent_tokenize(allSections):\n",
        "    if \"[[\" + subject + \"]]\" in sentence:\n",
        "      sentencesWithSubjectLine.append(sentence)  \n",
        "  \n",
        " \n",
        "  # Finds all NEs (that either look like [[c]] or [[a|b]]\n",
        "  # in the list of sentences with subject. \n",
        "  # Including NEs such as \"'s-Hertogenbosch\", \"'s Gravenshage\" and \"'t Vossenhol\".\n",
        "  # The Ł-character is added, because of a Polish city named in one of the articles.\n",
        "  NEsInSentencesLine = []\n",
        "  for sentence in sentencesWithSubjectLine:\n",
        "    NEsInSentencesLine.append(re.findall(r\"\\[\\[(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", sentence))\n",
        "     \n",
        "  # Changes all [[a|b]] in [[b]].\n",
        "  for listLinks in NEsInSentencesLine:\n",
        "    for link in listLinks:\n",
        "      if link != '':\n",
        "        if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "          linkAfter = re.sub(\"\\[\\[(.*?)\\|\", \"[[\", link)\n",
        "          allSections = re.sub(re.escape(link), linkAfter, allSections)\n",
        "  \n",
        "  # Makes a list of all sentences containing the subject,\n",
        "  # where links take the form of [[a]]. \n",
        "  sentencesWithSubject = []\n",
        "  for sentence in tokenize.sent_tokenize(allSections):\n",
        "    if \"[[\" + subject + \"]]\" in sentence:\n",
        "      sentencesWithSubject.append(sentence)\n",
        "      \n",
        "  # Creates the dictionary {named entity: label}.\n",
        "  NEplusTags = {}\n",
        "  for listLinks in NEsInSentencesLine:\n",
        "    for link in listLinks:\n",
        "      if link != '':\n",
        "        if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "          link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "          linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "          linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "          if getURL(linkBefore) != \" does not have a link.\":\n",
        "            NEplusTags[linkAfter] = getNERtag(linkBefore)   \n",
        "        else:\n",
        "          link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "          if getURL(link) != \" does not have a link.\":\n",
        "            NEplusTags[link] = getNERtag(link)\n",
        "  \n",
        "  return sentencesWithSubject, NEplusTags\n",
        "\n",
        "def cleanSection(section):\n",
        "  # Deletes the infobox, categories, files, links, subheaders, footnotes and images. eldingen van de sectietekst.\n",
        "  changes = [\n",
        "    ('\\{\\| class(.*?)\\|\\}', ''), \n",
        "    ('\\[\\[Categorie:(.*?)\\]\\]\\n?', ''),\n",
        "    ('\\[\\[Bestand(.*?)\\]\\](?:\\s-)?\\n', ''),\n",
        "    ('\\[\\[Bestand(.*?)\\]\\]$', ''),\n",
        "    ('\\[\\[Image:(.*?)\\]\\]\\n', ''),\n",
        "    ('\\[\\[Image:(.*?)\\]\\]', ''),\n",
        "    ('\\[\\[Afbeelding(.*?)\\]\\]\\n', ''),\n",
        "    ('\\[\\[File(.*?)\\]\\]\\n', ''),\n",
        "    ('<ref>\\[.*?\\n?.*?\\].*?<\\/ref>', ''),\n",
        "    ('<ref name=\"\\w*\"/>', ''),    \n",
        "    ('<\\/?(.*?)>(.*?)<\\/?(.*?)>', ''),\n",
        "    ('\\{\\{((.*?)(.+\\n)*)\\}\\}\\n*', ''),\n",
        "    ('==(.*?)==\\s', ''), \n",
        "    ('\\[http.*?\\]\\n?', ''),\n",
        "    ('\\*\\s?\\[http.*?\\]', ''), \n",
        "    (\"\\'\\'\\'\", \"\"),\n",
        "    (\"\\'\\'\", \"\"),\n",
        "    ('\\n',' ')\n",
        "  ]\n",
        "  \n",
        "  for old, new in changes:\n",
        "    section = re.sub(old, new, section) \n",
        "  \n",
        "  # Finds all [[a|b]] where b starts with a lowercase letter or integer. \n",
        "  # Deletes the a| in [[a|b]] links. \n",
        "  for link in re.findall(r\"\\[\\[([^]]+)\\|[a-z0-9].*?\\]\\]\", section):\n",
        "    section = re.sub(re.escape(link) + r\"\\|\", \"\", section)\n",
        "  \n",
        "  # Finds all links that start with a lowercase letter or integer, \n",
        "  # and deletes the [[]] around those links. \n",
        "  for link in re.findall(r\"\\[\\['?\\.?[a-z0-9].*?\\]\\]\", section):\n",
        "    link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "    section = re.sub(r\"\\[\\[\" + re.escape(link) + r\"\\]\\]\", re.escape(link), section)\n",
        "    \n",
        "  # Words after a bullet * always start with a capital letter,\n",
        "  # even though they do not refer to a named entity. \n",
        "  # Code below will delete the [[]] that follow after a *. \n",
        "  # This code needs improvement, \n",
        "  # because it will also delete the [[]] of a named entity after a *.\n",
        "  for link in re.findall(r\"\\*\\[\\[.*?\\]\\]\", section):\n",
        "    link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "    section = re.sub(r\"\\[\\[\" + re.escape(link) + r\"\\]\\]\", re.escape(link), section)\n",
        "    \n",
        "  # Sometimes the text contains words like [[Nederland]]se or [[Bloedbank]]en. \n",
        "  # The link will go to the article that is inside the [[]], \n",
        "  # but the NE is in that case not complete.  \n",
        "  # code below will change [[a]]b to [[ab]].\n",
        "  for link in re.findall(r\"\\[\\[(\\b[A-Z].*?)\\]\\]\", section):\n",
        "    for incomplNE in re.findall(r\"\\[\\[\" + re.escape(link) + r\"\\]\\][a-z]+\", section):\n",
        "      complNE = re.sub(\"[\\[\\[\\]\\]]\", \"\", incomplNE)\n",
        "      section = re.sub(re.escape(incomplNE), \"[[\" + complNE + \"]]\", section)\n",
        "  \n",
        "  # If the word is being linked to a non-existing page, the [[]] of the link are deleted.\n",
        "  for link in re.findall(r\"\\[\\[\\s?(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", section):\n",
        "    print(link)\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "      linkClean = re.sub(\"[\\[\\[\\]\\]]\", \"\", linkBefore)\n",
        "      if getURL(linkClean) == \" does not have a link.\":\n",
        "        section = re.sub(re.escape(linkBefore), linkClean, section)\n",
        "    else:\n",
        "      linkClean = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "      if getURL(linkClean) == \" does not have a link.\":\n",
        "        section = re.sub(re.escape(link), linkClean, section)\n",
        "      \n",
        "  return section\n",
        "\n",
        "def getFirstSection(subject):\n",
        "  linksInSectionURL = []\n",
        "  PARAMS = {\n",
        "  'action': \"parse\",\n",
        "  'prop': \"wikitext\",\n",
        "  'section': 0,\n",
        "  'page': subject,\n",
        "  'format': \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  section = data['parse']['wikitext']['*']\n",
        "\n",
        "  section = cleanSection(section)\n",
        "\n",
        "  # The title of the article is also a NE, so it is put between [[]].\n",
        "  section = re.sub(re.escape(subject), \"[[\" + subject + \"]]\", section)\n",
        "    \n",
        "  linksInSection = re.findall(r\"\\[\\[(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", section)\n",
        "  \n",
        "  # All NEs are stripped from [[]] and added to the list linksInSectionURL.  \n",
        "  for link in linksInSection:\n",
        "    link = re.sub(r\"\\[\\[|\\]\\]\", \"\", link)\n",
        "    linksInSectionURL.append(link)\n",
        "  \n",
        "  # If NEs in linksInSectionURL take the form of [[a|b]], a| will be deleted, \n",
        "  # so their form will be [[b]].\n",
        "  for link in linksInSectionURL:\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "      section = re.sub(re.escape(link), linkAfter, section)\n",
        "  \n",
        "  # The dictionary {named entity: label} is created.\n",
        "  NEplusTags = {}\n",
        "  for link in linksInSectionURL:\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "      linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "      if getURL(linkBefore) != \" does not have a link.\":\n",
        "        tag1 = getNERtag(linkBefore) \n",
        "        NEplusTags[linkAfter]= tag1\n",
        "    elif getURL(link) != \" does not have a link.\":\n",
        "      tag2 = getNERtag(link)\n",
        "      NEplusTags[link] = tag2  \n",
        "\n",
        "  return (section, NEplusTags)\n",
        "\n",
        "# Determines the category of a named entity.\n",
        "queue = []\n",
        "def getNERtag(subject):\n",
        "  result = ''\n",
        "  queue.append(subject)\n",
        "  \n",
        "  while queue and result == '':\n",
        "    cat = queue.pop(0)\n",
        "    result = findCat(cat)\n",
        "\n",
        "  return result\n",
        "      \n",
        "  \n",
        "def findCat(subject): \n",
        "  # Retrieves the parent categories of a certain Wikipedia article. \n",
        "  PARAMS = {\n",
        "      'action': 'query',\n",
        "      'format': 'json',\n",
        "      'pageids': getURL(subject),\n",
        "  }\n",
        "  \n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  title = data['query']['pages'][getURL(subject)]['title']\n",
        "  \n",
        "  parentCategories = getParentCat(title)\n",
        "  \n",
        "  if parentCategories == subject + \" does not have any parent categories.\":\n",
        "    return 'misc'\n",
        "  \n",
        "  sti = ['stichting']\n",
        "  org = ['organisatie', 'vereniging', 'kerkgenootschap', 'fonds', 'ministerie', \n",
        "        'partij', 'omroep', 'krijgsmacht', 'universiteit', 'bestuursorgaan', 'raad', 'instituut',\n",
        "        'museum', 'instantie', 'bedrijf']\n",
        "  per = ['persoon', 'hoogleraar', 'wetenschapper', 'schrijver', 'politicus',\n",
        "        'ontwerper', 'god ']\n",
        "  loc = ['geografie', 'plaats', 'rijk', 'continent', 'provincie', \n",
        "        'meer', 'land in', 'monument', 'bouwwerk', 'planeet']\n",
        "  eve = ['oorlog', 'themadag', 'evenement', 'natuurramp']\n",
        "  pro = ['taal', 'dialect', 'prijs', 'boek ', 'schrift', ' dier', 'object', 'lied', \n",
        "         'voertuig', 'vliegtuig', 'krant']\n",
        "  misc = ['stroming', 'volk', 'formaat', 'chemische stof', 'wet ', 'koninkrijk',\n",
        "         'website', 'onderwijsvorm']\n",
        "  \n",
        "  for cat in parentCategories:\n",
        "    for foundation in sti:\n",
        "      if foundation in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'sti'\n",
        "  \n",
        "  for cat in parentCategories:\n",
        "    for organisation in org:\n",
        "      if organisation in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'org'\n",
        "    for person in per:\n",
        "      if person in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'per'\n",
        "    for location in loc:\n",
        "      if location in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'loc'\n",
        "    for event in eve:\n",
        "      if event in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'eve'\n",
        "    for product in pro:\n",
        "      if product in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'pro'\n",
        "    for miscellaneous in misc:\n",
        "      if miscellaneous in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'misc'\n",
        "\n",
        "  queue.extend(parentCategories)\n",
        "  \n",
        "  return ''\n",
        "\n",
        "# Will get the URL of a certain Wikipedia-article  \n",
        "def getURL(subject):\n",
        "  PARAMS = {\n",
        "      'action': \"query\",\n",
        "      'titles': subject,\n",
        "      'format': \"json\",\n",
        "      'redirects': True\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  \n",
        "  for pageid in data['query']['pages'].keys():\n",
        "    if pageid == \"-1\":\n",
        "      return \" does not have a link.\"\n",
        "    else:\n",
        "      return pageid\n",
        "\n",
        "def cleanWpL(text):  \n",
        "  changes = [\n",
        "    (r\"([?!:/;\\\"]+)\", r\" \\1\"), \n",
        "    (r\"([()’‘]+)\", r\" \\1 \"),\n",
        "    (r\"([.,]\\s)\", r\" \\1 \"),\n",
        "    (r\"\\.$\", r\" . \"),\n",
        "    (r\"\\\\\", \"\"),\n",
        "    (r\"\\]\\]'\", \"]] '\"), # for 't Vossenhol\n",
        "    (\"  \", \" \")\n",
        "  ]\n",
        "  \n",
        "  for old, new in changes:\n",
        "    text = re.sub(old, new, text) \n",
        "  \n",
        "  # Deals with NEs that look like [[a]]-[[b]], [[a]]-b, a-[[b]].\n",
        "  NEsHyphen = re.findall(r\"\\[\\[.*?\\]\\]-(?:\\[\\[)*\\w*(?:\\]\\])*\", text)\n",
        "  for ne in NEsHyphen:\n",
        "    neNew = re.sub(r\"-\", \" - \", ne)\n",
        "    text = re.sub(re.escape(ne), neNew, text)\n",
        "   \n",
        "  # Deals with abbreviations.\n",
        "  abbreviations = re.findall(r\"[A-Z]\\s\\.(?:[A-Z]\\s\\.)*\\s\\w*\", text)\n",
        "  for abrv in abbreviations:\n",
        "    abrvNew = re.sub(r\"\\s\\.\", \".\", abrv)\n",
        "    text = re.sub(re.escape(abrv), abrvNew, text)\n",
        "    \n",
        "  # Deals with various non-alphanumeric characters inside of NEs.\n",
        "  allNEs = re.findall(r\"\\[\\[(?:.*?)\\]\\]\", text)\n",
        "  for ne in allNEs:\n",
        "    if re.search(\"\\(.*?\\)\", ne):\n",
        "      neNew = re.sub(\"\\s\\)\\s\", \")\", ne)\n",
        "      neNewer = re.sub(\"\\(\\s\", \"(\", neNew)\n",
        "      text = re.sub(re.escape(ne), neNewer, text)\n",
        "    if ' - ' in ne:\n",
        "      neNew = re.sub(\"[\\[\\[\\]\\]]\", \"\", ne)\n",
        "      if neNew not in dictOfNEsAndTags:\n",
        "        neNew = re.sub('\\s-\\s', '-', ne)\n",
        "        text = re.sub(re.escape(ne), neNew, text)\n",
        "    if re.search(\"\\[\\[\\[\\[.*?\\]\\]\", ne):\n",
        "      neNew = re.sub(\"[\\[\\[\\]\\]]\", \"\", ne)\n",
        "      text = re.sub(re.escape(ne), neNew, text)\n",
        "    if re.search(re.escape(ne) + \"\\]\\]\", text):\n",
        "      text = re.sub(\"[\\]\\]]\", \"\", text)\n",
        "    for n in re.findall(r\"\\s[?!:,\\\".’/;]+\", ne):\n",
        "      neNew = re.sub(r\"\\s\", \"\", n)\n",
        "      neNewer = re.sub(re.escape(n), neNew, ne)\n",
        "      text = re.sub(re.escape(ne), neNew, text)\n",
        "      \n",
        "  return text\n",
        "\n",
        "# Will turn a text into a token-per-line format.\n",
        "\n",
        "'''\n",
        "Because of the use of a dictionary {named entity: label}, \n",
        "named entities that have different labels in different contexts, \n",
        "such as 'Bart Smit' either being a PER or an ORG, \n",
        "will only have one key and one value. \n",
        "So, if the first occurence of 'Bart Smit' is annotated as ORG, \n",
        "all the other occurences in the text will also be annotated as ORG.\n",
        "Instead of a dictionary, an array would be a better option.\n",
        "'''\n",
        "def wordPerLine(text, dictOfNEsAndTags):\n",
        "  print(text)\n",
        "  print(dictOfNEsAndTags)\n",
        "  \n",
        "  # Cleans the text so that it is ready to be tokenized. \n",
        "  text = cleanWpL(text)\n",
        "  \n",
        "  # Splits the text in sentences.\n",
        "  listOfSentences = re.split(r'(?<= \\.) ', text)\n",
        "\n",
        "  output = []\n",
        "  search = \"\"\n",
        "  inTag = False\n",
        "  \n",
        "  # Will add the BIO-encoding.\n",
        "  for sentence in listOfSentences:\n",
        "    for w in sentence.split(\" \"):\n",
        "      outTag = False\n",
        "      rest = w\n",
        "\n",
        "      if rest[:2] == \"[[\":\n",
        "        rest = rest[2:]\n",
        "        inTag = True\n",
        "      if rest[-2:] == \"]]\":\n",
        "        rest = rest[:-2]\n",
        "        outTag = True\n",
        "\n",
        "      if inTag:\n",
        "        search += rest\n",
        "        if outTag:\n",
        "          val = dictOfNEsAndTags[search]\n",
        "          for word in search.split()[:1]:\n",
        "            output.append(word + \"\\tB-\" + val)\n",
        "          for word in search.split()[1:]:\n",
        "            output.append(word + \"\\tI-\" + val)\n",
        "          inTag = False\n",
        "          search = \"\"\n",
        "        else:\n",
        "          search += \" \"\n",
        "      \n",
        "      elif rest != \"\":\n",
        "        output.append(rest + \"\\tO\")\n",
        "    \n",
        "    if output:\n",
        "      if output[-1].endswith('.\\tO'):\n",
        "        output.extend(\"\\n\")\n",
        " \n",
        "  return output\n",
        "\n",
        "'''\n",
        "for foundation in getCategorieLijst(\"Surinaamse stichting\"):\n",
        "  for sentence in getSentencesWithSubject(foundation)[0]:\n",
        "    for w in wordPerLine(sentence, getSentencesWithSubject(foundation)[1]):\n",
        "      g.write(w)\n",
        "      g.write(\"\\n\")\n",
        "    g.write(\"\\n\")\n",
        "    \n",
        "\n",
        "g = open('zinnenmetonderwerp.txt')\n",
        "inhoud = g.read()\n",
        "g.close()\n",
        "nieuweInhoud = inhoud.replace('\\n\\n\\n', '\\n')\n",
        "g = open('zinnenmetonderwerp.txt', 'w')\n",
        "g.write(nieuweInhoud)\n",
        "\n",
        "\n",
        "\n",
        "# Duration 1 hour and 45 minutes for 440 articles.\n",
        "for foundation in getCategorieLijst(\"Nederlandse stichting\"):\n",
        "  for w in wordPerLine(getFirstSection(foundation)[0], getFirstSection(foundation)[1]):\n",
        "    for x in w:\n",
        "      f.write(x)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "f = open('firstsections.txt')\n",
        "content = f.read()\n",
        "f.close()\n",
        "newContent = content.replace('\\n\\n', '\\n')\n",
        "f = open('firstsections.txt', 'w')\n",
        "f.write(newContent)\n",
        "'''\n",
        "\n",
        "g.close()\n",
        "f.close()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[Nederlandse]]\n",
            "[[John Blankenstein]]\n",
            "[[Gelijkspel (boek)|Gelijkspel, portretten van homo topsporters]]\n",
            "[[Canal Parade]]\n",
            "[[ Alliantie Gelijkspelen]]\n",
            "[[ASV De Dijk]]\n",
            "[[Nederlandse]]\n",
            "[[John Blankenstein]]\n",
            "[[Gelijkspel (boek)|Gelijkspel, portretten van homo topsporters]]\n",
            "[[Canal Parade]]\n",
            "[[ Alliantie Gelijkspelen]]\n",
            "[[ASV De Dijk]]\n",
            "De [[John Blankenstein Foundation]] (JBF) is een [[Nederlandse]] stichting die zich ten doel stelt om de sociale acceptatie van homo\\'s en lesbiennes in de top- en breedtesport te bevorderen. Specifiek richt de [[John Blankenstein Foundation]] zich onder meer op het verbeteren van zichtbaarheid en beeldvorming van homoseksuele sporters en door het bevorderen van best practices in bonds-, club- en ongeorganiseerde sportverbanden.  De [[John Blankenstein Foundation]] werd opgericht op 18 december 2008 en is vernoemd naar de Nederlandse voetbalscheidsrechter en activist voor homorechten [[John Blankenstein]]. Het initiatief tot de oprichting van het stichting kwam van Blankensteins zus Karin Nederpelt-Blankenstein.  De JBF publiceerde mede het boek [[Gelijkspel, portretten van homo topsporters]]. De organisatie nam deel aan de boot \"Winnen met respect\" tijdens de [[Canal Parade]] in 2009 en financierde mede een onderzoek naar drempels in de mannelijke teamsporten.   Sinds 2008 werkt de JBF samen met zes sport- en homo-organisaties in de  Alliantie Gelijkspelen. [[ASV De Dijk]] is de eerste voetbalvereniging in Nederland die een samenwerking is aangegaan met de JBF.\n",
            "{'John Blankenstein Foundation': 'sti', 'Nederlandse': 'loc', 'John Blankenstein': 'per', 'Gelijkspel, portretten van homo topsporters': 'pro', 'Canal Parade': 'eve', 'ASV De Dijk': 'org'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Ze6Nx9QQcGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}